{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/labasi/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thresholding number of glyphs per sign at 50...\n",
      "Thresholding finished.\n",
      "\n",
      "total train num: 2844\n",
      "total val num: 158\n",
      "total test num: 179\n",
      "\n",
      "No of sign groups: 43\n",
      "\n",
      "Shuffling...\n",
      "Shuffling finished.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "ACDH LBASI CNN\n",
    "17 Aug 2018\n",
    "\n",
    "# Labasi CNN Work Flow\n",
    "\n",
    "The purpose of this project was to prepare code for training a CNN for recognizing glyphs from the \n",
    "labasi database as their respective signs. Due to time constraints, thorough training was not possible, \n",
    "but a workflow with snippets of code is provided below. Of particular note for those interested is \n",
    "the section on writing and reading tfrecord files (formatting one's own image files for use in tensorflow) \n",
    "and preparing one-hots for the image files.\n",
    "\n",
    "## File Preparation\n",
    "\n",
    "Because there are cases of signs with very few instances of glyphs, a threshold of glyphs-per-sign should be\n",
    "used to prepare the files. In this case, 50 was used. Each sign group is assigned an integer within the \n",
    "range of total groups that qualify over the threshold. This integer is later used as an id and turned into a \n",
    "one-hot for training.\n",
    "'''\n",
    "df1 = pd.read_csv('/Volumes/IMVDrive/cfdb-django/glyphs-aligned-w-std_sign-images.csv', usecols=['sign', 'glyph'])\n",
    "df_group = df1.groupby(by=['sign'])\n",
    "df_group = sorted(df_group, key=lambda x: len(x[1])) #https://stackoverflow.com/questions/22291395/sorting-the-grouped-data-as-per-group-size-in-pandas\n",
    "\n",
    "train_list = pd.DataFrame(data=None, columns=['sign', 'glyph','onehot'])\n",
    "val_list = pd.DataFrame(data=None, columns=['sign', 'glyph','onehot'])\n",
    "test_list = pd.DataFrame(data=None, columns=['sign', 'glyph','onehot'])\n",
    "\n",
    "group_count = 0\n",
    "print(\"Thresholding number of glyphs per sign at 50...\")\n",
    "first = True\n",
    "for name,group in df_group:\n",
    "    if len(group) >= 50:\n",
    "        group_count = group_count+1\n",
    "        \n",
    "        # id column of identical integers to identify each instance of the group\n",
    "        col = []\n",
    "        for g in range(len(group)):\n",
    "            col.append(group_count)\n",
    "        col_df = pd.DataFrame(col, columns=['onehot'])\n",
    "        \n",
    "        # assign sections of the column to train, validate, and test groups\n",
    "        train_col = col[0:int(0.9*len(group))]\n",
    "        val_col = col[int(0.9*len(group)):int(0.95*len(group))]\n",
    "        test_col = col[int(0.95*len(group)):]\n",
    "        \n",
    "        # assign sections of the orginal group to train, validate, and test groups\n",
    "        train_group = group[0:int(0.9*len(group))] \n",
    "        val_group = group[int(0.9*len(group)):int(0.95*len(group))]\n",
    "        test_group = group[int(0.95*len(group)):]\n",
    "        \n",
    "        # join the onehot column to the original group\n",
    "        train_group = train_group.assign(onehot=train_col)\n",
    "        val_group = val_group.assign(onehot=val_col)\n",
    "        test_group = test_group.assign(onehot=test_col)\n",
    "        \n",
    "        # for each successive group append the data to the growing list of train, validate, and test groups\n",
    "        train_list = pd.concat([train_list, train_group], join_axes=[train_list.columns], ignore_index=True)\n",
    "        val_list = pd.concat([val_list, val_group], join_axes=[val_list.columns], ignore_index=True)\n",
    "        test_list = pd.concat([test_list, test_group], join_axes=[test_list.columns], ignore_index=True)\n",
    "        \n",
    "print(\"Thresholding finished.\")\n",
    "print(\"\")\n",
    "print(\"total train num: \"+str(len(train_list)))\n",
    "print(\"total val num: \"+str(len(val_list)))\n",
    "print(\"total test num: \"+str(len(test_list)))\n",
    "\n",
    "batch_file_names = ['/Volumes/imvDrive/cfdb-django/media/train_batch.csv', \n",
    "                    '/Volumes/imvDrive/cfdb-django/media/validation_batch.csv', \n",
    "                    '/Volumes/imvDrive/cfdb-django/media/testing_batch.csv']\n",
    "\n",
    "# write finished train, validate, and test groups to csv files\n",
    "train_list.to_csv(batch_file_names[0])\n",
    "val_list.to_csv(batch_file_names[1])\n",
    "test_list.to_csv(batch_file_names[2])\n",
    "print(\"\")\n",
    "print(\"No of sign groups: \"+str(group_count))\n",
    "print(\"\")\n",
    "\n",
    "# shuffle the train, validate, and test groups\n",
    "print(\"Shuffling...\")\n",
    "for i in range(len(batch_file_names)):\n",
    "    f = open(batch_file_names[i], \"r\")\n",
    "    lines = f.readlines()\n",
    "    l = lines[1:]\n",
    "    f.close() \n",
    "    random.shuffle(l)\n",
    "\n",
    "    f = open(batch_file_names[i], \"w\")  \n",
    "    f.write(',sign,glyph,onehot\\n')\n",
    "    f.writelines(l)\n",
    "    f.close()\n",
    "print(\"Shuffling finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating train.tfrecords...\n",
      "train.tfrecords data: 0/2844\n",
      "Corrupted record...\n",
      "Corrupted record...\n",
      "Corrupted record...\n",
      "train.tfrecords data: 1000/2844\n",
      "Corrupted record...\n",
      "Corrupted record...\n",
      "Corrupted record...\n",
      "train.tfrecords data: 2000/2844\n",
      "Corrupted record...\n",
      "Corrupted record...\n",
      "Corrupted record...\n",
      "\n",
      "Creating val.tfrecords...\n",
      "val.tfrecords data: 0/158\n",
      "Corrupted record...\n",
      "\n",
      "Creating test.tfrecords...\n",
      "test.tfrecords data: 0/179\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "## ------------------------------------------------------------\n",
    "## FORMAT CUTOM IMAGE DATA SET\n",
    "## --source: \n",
    "## Daniel Persson,'How to load a custom dataset with tf.data [Tensorflow]',\n",
    "## https://www.youtube.com/watch?v=bqeUmLCgsVw\n",
    "## ------------------------------------------------------------\n",
    "## I use PIL instead of cv2\n",
    "## ------------------------------------------------------------\n",
    "\n",
    "label_ids_list = []\n",
    "\n",
    "# a function for formatting a list of integers into a train feature\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "# a function for formatting a list of bytes into a train feature\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def createDataRecord(outFileName, addrs, labels):\n",
    "    print(\"\")\n",
    "    print('Creating '+outFileName+'...')\n",
    "    \n",
    "    #open a writer\n",
    "    writer = tf.python_io.TFRecordWriter(outFileName)\n",
    "    for i in range(len(addrs)):\n",
    "        # check every 1000 rows has been written\n",
    "        if not i % 1000:\n",
    "            print(outFileName+' data: {}/{}'.format(i,len(addrs)))\n",
    "            sys.stdout.flush() \n",
    "        # with the image name from the provided address list, load the image from its directory\n",
    "        filename = os.fsdecode('/Volumes/imvDrive/cfdb-django/media/glyph_img/'+addrs.iloc[i][0])\n",
    "        if Path(filename).is_file():\n",
    "            try:\n",
    "                \n",
    "                \n",
    "                # Artifical aplification of the dataset should occur here. Due to time constraints\n",
    "                # ...further amplification was not finished, but worthwhile amplification would include different\n",
    "                # ...hues, degrees of noise, and image sharpness.\n",
    "\n",
    "                # convert to gray scale\n",
    "                img = Image.open(filename).convert('L') \n",
    "                img = img.resize((331,331)) \n",
    "                label = labels.iloc[i][0]\n",
    "                \n",
    "                feature_img = {\n",
    "                    'glyph_img_raw': _bytes_feature(img.tobytes()),\n",
    "                    'label_raw': _int64_feature(label)\n",
    "                }\n",
    "\n",
    "                example = tf.train.Example(features=tf.train.Features(feature=feature_img))\n",
    "                writer.write(example.SerializeToString())\n",
    "            \n",
    "            \n",
    "            # A small number of images were corrupted and given the \n",
    "            # ...infrequency of corruption it is possible to skip instances \n",
    "            \n",
    "            except Exception:\n",
    "                print(\"Corrupted record...\")\n",
    "                pass\n",
    "    writer.close()\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# load the list of filenames (addresses) and integer ids (labels that will be converted to onehots) from the csv files\n",
    "train_addrs = pd.read_csv('/Volumes/IMVDRIVE/cfdb-django/media/train_batch.csv', usecols=['glyph'])\n",
    "train_labels = pd.read_csv('/Volumes/IMVDRIVE/cfdb-django/media/train_batch.csv', usecols=['onehot'])\n",
    "val_addrs = pd.read_csv('/Volumes/IMVDRIVE/cfdb-django/media/validation_batch.csv', usecols=['glyph'])\n",
    "val_labels = pd.read_csv('/Volumes/IMVDRIVE/cfdb-django/media/validation_batch.csv', usecols=['onehot'])\n",
    "test_addrs = pd.read_csv('/Volumes/IMVDRIVE/cfdb-django/media/testing_batch.csv', usecols=['glyph'])\n",
    "test_labels = pd.read_csv('/Volumes/IMVDRIVE/cfdb-django/media/testing_batch.csv', usecols=['onehot'])\n",
    "\n",
    "train_num_examples = len(train_addrs)\n",
    "val_num_examples = len(val_addrs)\n",
    "test_num_examples = len(test_addrs)\n",
    "\n",
    "createDataRecord('train.tfrecords', train_addrs, train_labels)\n",
    "createDataRecord('val.tfrecords', val_addrs, val_labels)\n",
    "createDataRecord('test.tfrecords', test_addrs, test_labels)\n",
    "\n",
    "# this file is not used in this program, but may be useful\n",
    "labels = ['ascii', 'sign_name', 'sign_img']\n",
    "label_ids_df = pd.DataFrame.from_records(label_ids_list, columns=labels)\n",
    "label_ids_df.to_csv('label_ids.csv')\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## ------------------------------\n",
    "## READ INPUT: version 1\n",
    "## --source: \n",
    "## Daniel Persson,'How to load a custom dataset with tf.data [Tensorflow]',\n",
    "## https://www.youtube.com/watch?v=bqeUmLCgsVw\n",
    "## ------------------------------\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "channels = 1\n",
    "batch_size = 5\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "def parser(record):\n",
    "    keys_to_features = {\n",
    "        \"glyph_img_raw\": tf.FixedLenFeature([], tf.string),\n",
    "        \"label_raw\": tf.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    \n",
    "    parsed = tf.parse_single_example(record, keys_to_features)\n",
    "    image = tf.decode_raw(parsed[\"glyph_img_raw\"], tf.uint8)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.reshape(image, shape=[331, 331, channels])\n",
    "    label = tf.cast(parsed[\"label_raw\"], tf.int32)\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "def input_fn(filenames, train, batch_size=batch_size, buffer_size=2048):\n",
    "    dataset = tf.data.TFRecordDataset(filenames=filenames)\n",
    "    dataset = dataset.map(parser)\n",
    "    \n",
    "    if train:\n",
    "        dataset = dataset.shuffle(buffer_size=buffer_size)\n",
    "        num_repeat = None\n",
    "    else:\n",
    "        num_repeat = 1\n",
    "        \n",
    "    dataset = dataset.repeat(num_repeat)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    images_batch, labels_batch = iterator.get_next()\n",
    "    x = {'image': images_batch}\n",
    "    y = labels_batch\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def train_input_fn():\n",
    "    return input_fn(filenames=['/Volumes/IMVDRIVE/cfdb-django/train.tfrecords'], train=True)\n",
    "\n",
    "def val_input_fn():\n",
    "    return input_fn(filenames=[\"/Volumes/IMVDRIVE/cfdb-django/val.tfrecords\"], train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-6dfe05570e2f>:8: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n",
      "PROCESSING 568 training epochs...\n",
      "Training Epoch 1 --- Training Accuracy:   0.0%, Validation Accuracy:   0.0%,  Validation Loss: 3.795\n",
      "Training Epoch 2 --- Training Accuracy:   0.0%, Validation Accuracy:   0.0%,  Validation Loss: 3.773\n",
      "Training Epoch 3 --- Training Accuracy:   0.0%, Validation Accuracy:   0.0%,  Validation Loss: 3.782\n",
      "Training Epoch 4 --- Training Accuracy:  20.0%, Validation Accuracy:   0.0%,  Validation Loss: 3.788\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6dfe05570e2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mone_hot_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_train_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup_count\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mfd_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_train_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mone_hot_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfd_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mbatch_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_counter\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/labasi/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/labasi/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/labasi/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/labasi/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/labasi/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## ------------------------------\n",
    "## VARIABLES\n",
    "## ------------------------------\n",
    "\n",
    "sess = tf.Session()\n",
    "x = tf.placeholder(tf.float32, shape=[batch_size, 331, 331, 1])\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, group_count+1], name='y_true')\n",
    "y_true_cls = tf.argmax(y_true, dimension=1)\n",
    "\n",
    "## ------------------------------\n",
    "## CONVOLUTIONAL NEURAL NETWORK\n",
    "## Adapted from:\n",
    "## https://cv-tricks.com/tensorflow-tutorial/training-convolutional-neural-network-for-image-classification/\n",
    "## https://www.tensorflow.org/tutorials/images/deep_cnn\n",
    "## ------------------------------\n",
    "\n",
    "filter_shape = 5\n",
    "\n",
    "def conv_layer1_fn(x,num_input_channels, num_filters):\n",
    "    W1 = tf.Variable(tf.truncated_normal([filter_shape,filter_shape,num_input_channels,num_filters], dtype=tf.float32))     \n",
    "    B1 = tf.Variable(tf.truncated_normal([num_filters]))\n",
    "    conv1 = tf.nn.conv2d(x, W1, strides=[1, 1, 1, 1], padding=\"SAME\") #stides=[batch_stride x_stride y_stride depth_stride]\n",
    "    biased_conv1 = conv1+B1\n",
    "    relu_for_conv = tf.nn.relu(biased_conv1)\n",
    "    pool1 = tf.nn.max_pool(relu_for_conv, ksize=[1, 2, 2, 1], strides=[1,2,2,1], padding='SAME')\n",
    "    norm1 = tf.nn.local_response_normalization(pool1,depth_radius=5,bias=1,alpha=1,beta=0.5,name=None)\n",
    "    return norm1\n",
    "    \n",
    "def conv_layer2_fn(layer, num_input_channels, num_filters):\n",
    "    W2 = tf.Variable(tf.truncated_normal([filter_shape,filter_shape,num_input_channels,num_filters], stddev=0.05)) #https://cv-tricks.com/tensorflow-tutorial/training-convolutional-neural-network-for-image-classification/\n",
    "    B2 = tf.Variable(tf.truncated_normal([num_filters]))\n",
    "    conv2 = tf.nn.conv2d(layer, W2, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    biased_conv2 = conv2+B2\n",
    "    relu_for_conv = tf.nn.relu(biased_conv2)\n",
    "    norm2 = tf.nn.local_response_normalization(relu_for_conv,depth_radius=5,bias=1,alpha=1,beta=0.5,name=None)\n",
    "    pool2 = tf.nn.max_pool(norm2, ksize=[1, 2, 2, 1], strides=[1,2,2,1], padding='SAME')\n",
    "    return pool2\n",
    "\n",
    "def flat_layer(layer):                  \n",
    "    layer_shape = layer.get_shape()\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    layer = tf.reshape(layer, [-1, num_features])\n",
    "    return layer \n",
    "\n",
    "def fc_relu(layer, use_relu, output_num, reshape):\n",
    "    layer_shape = layer.get_shape()\n",
    "    W3 = tf.Variable(tf.truncated_normal([layer_shape[1:4].num_elements(), output_num], stddev=0.05))\n",
    "    B3 = tf.Variable(tf.truncated_normal([output_num],stddev=0.03))\n",
    "    std_hypothesis = tf.matmul(layer, W3) + B3\n",
    "\n",
    "    if reshape == True:\n",
    "        reshaped_std_hypothesis = tf.reshape(std_hypothesis, [batch_size, output_num])\n",
    "        return reshaped_std_hypothesis\n",
    "    if use_relu == True:\n",
    "        fc_relu = tf.nn.relu(std_hypothesis)\n",
    "        return fc_relu\n",
    "    else:\n",
    "        print(\"OOOPs\")\n",
    "        return std_hypothesis\n",
    "    \n",
    "    \n",
    "conv_layer1 = conv_layer1_fn(x, 1, 32)\n",
    "conv_layer2 = conv_layer2_fn(conv_layer1, 32, 64)\n",
    "flat = flat_layer(conv_layer2)\n",
    "fc_relu1 = fc_relu(flat, use_relu=True, output_num=batch_size, reshape=False)\n",
    "fc_relu2 = fc_relu(fc_relu1, use_relu=False, output_num=group_count+1, reshape=True)\n",
    "    \n",
    "## ------------------------------\n",
    "## OPTIMIZATION SESSION\n",
    "## Adapted from:\n",
    "## https://cv-tricks.com/tensorflow-tutorial/training-convolutional-neural-network-for-image-classification/\n",
    "## ------------------------------\n",
    "    \n",
    "prediction_per_class = tf.nn.softmax(fc_relu2) #y_pred\n",
    "predicted_class = tf.argmax(prediction_per_class, axis=1)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=fc_relu2,labels=y_true)\n",
    "cost = tf.reduce_mean(cross_entropy)   \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)\n",
    "correct_prediction = tf.equal(predicted_class, y_true_cls)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "def show_progress(epoch, feed_dict_train, feed_dict_validate, val_loss):\n",
    "    tr_acc = sess.run(accuracy, feed_dict=feed_dict_train)\n",
    "    val_acc = sess.run(accuracy, feed_dict=feed_dict_validate)\n",
    "    msg = \"Training Epoch {0} --- Training Accuracy: {1:>6.1%}, Validation Accuracy: {2:>6.1%},  Validation Loss: {3:.3f}\"\n",
    "    print(msg.format(epoch + 1, tr_acc, val_acc, val_loss))\n",
    "\n",
    "\n",
    "## ------------------------------\n",
    "## TRAINING\n",
    "## ------------------------------\n",
    "f_train, l_train = train_input_fn()\n",
    "f_val, l_val = val_input_fn()\n",
    "batch_counter = 0\n",
    "epoch_counter = 0\n",
    "epochs = int(train_num_examples/(val_num_examples/batch_size))\n",
    "train_batch_num = int(train_num_examples/batch_size)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "print(\"PROCESSING \"+str(train_batch_num)+\" training epochs...\")\n",
    "for b in range(train_batch_num):\n",
    "\n",
    "        x_train_batch, label_train_batch = sess.run([f_train['image'], l_train])\n",
    "        one_hot_train = tf.one_hot(label_train_batch, depth=group_count+1)\n",
    "        fd_train = {x: x_train_batch, y_true: one_hot_train.eval(session=sess)}\n",
    "        sess.run(optimizer, feed_dict=fd_train)\n",
    "        batch_counter = batch_counter+1\n",
    "        \n",
    "        if batch_counter % epochs == 0: \n",
    "            ## ------------------------------\n",
    "            ## VALIDATING\n",
    "            ## ------------------------------\n",
    "            x_valid_batch, label_valid_batch = sess.run([f_val['image'], l_val]) \n",
    "            one_hot_valid = tf.one_hot(label_valid_batch, depth=group_count+1)\n",
    "            fd_val ={x: x_valid_batch, y_true: one_hot_valid.eval(session=sess)}\n",
    "            val_loss = sess.run(cost, feed_dict=fd_val) \n",
    "            show_progress(epoch_counter, fd_train, fd_val, val_loss)\n",
    "            epoch_counter = epoch_counter+1\n",
    "            saver.save(sess, '/Volumes/imvDrive/cfdb-django/labasi_cnn')\n",
    "\n",
    "    \n",
    "saver = tf.train.import_meta_graph('/Volumes/imvDrive/cfdb-django/labasi_cnn.meta')\n",
    "saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "\n",
    "## ------------------------------\n",
    "## TESTING\n",
    "## ------------------------------\n",
    "test_counter = 0\n",
    "f_test, l_test = test_input_fn()\n",
    "\n",
    "while f_test != None or l_test != None:\n",
    "    try:\n",
    "        x_test_batch, label_test_batch = sess.run([f_test['image'], l_test])\n",
    "        one_hot_test = tf.one_hot(label_test_batch, depth=group_count+1)\n",
    "        feed_dict_testing = {x: x_test_batch, y_true: one_hot_test.eval(session=sess)}\n",
    "        result=sess.run(prediction_per_class, feed_dict=feed_dict_testing)\n",
    "        print(\"Epoch \"+str(test_counter*100)+\"%\"+\" / \"+str(result[0]))\n",
    "        test_counter = test_counter+1\n",
    "    except Exception:\n",
    "        pass\n",
    "print(\"FINISHED\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
